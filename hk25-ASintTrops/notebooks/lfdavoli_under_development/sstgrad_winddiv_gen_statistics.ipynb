{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d75a30cb-485a-42ce-a36b-7db19d3d4ee0",
   "metadata": {},
   "source": [
    "# Compare correlation of $\\nabla SST$ and $div(\\vec{U})$ for different env conds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c556dad-04d6-4f75-b8c2-0caf031f1a83",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "895b467c-1b62-4b8a-89eb-8518046106bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import intake\n",
    "from easygems import healpix as egh\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cf\n",
    "import cmocean\n",
    "import healpy as hp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import math\n",
    "from scipy.stats import sem\n",
    "from scipy.special import gammaincc\n",
    "from pycoare import coare_35\n",
    "import sys\n",
    "sys.path.append('/home/b/b383497/hk25-teams/hk25-ShallowCirc/src/')\n",
    "from toolbox import attach_coords, compute_hder, compute_conv, nest2ring_index\n",
    "sys.path.append('/home/b/b383497/hk25-ASintTrops/Scripts/lfdavoli/')\n",
    "import geometry as gm\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning) # don't warn us about future package conflicts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b753f827-119d-4f78-bda2-21de623030bf",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f49ba5a2-922f-4ce0-9b3c-1cf9dda293fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cells_of_region(ds, region):\n",
    "    if ('lon' not in list(ds.coords)) | ('lat' not in list(ds.coords)):\n",
    "        raise NameError('Missing coordinates')\n",
    "        \n",
    "    return np.where(\n",
    "        (ds.lon>np.mod(regions[region]['boundaries'][0],360)) & \n",
    "        (ds.lon<np.mod(regions[region]['boundaries'][1],360)) &\n",
    "        (ds.lat>regions[region]['boundaries'][2]) & \n",
    "        (ds.lat<regions[region]['boundaries'][3])\n",
    "    )[0]\n",
    "\n",
    "def get_region(ds,region):\n",
    "    if 'value' in list(ds.dims):\n",
    "        return ds.isel(value=cells_of_region(ds, region))        \n",
    "    elif 'cell' in list(ds.dims):\n",
    "        return ds.isel(cell=cells_of_region(ds, region))\n",
    "    \n",
    "\n",
    "def worldmap(var, extent=None,title=None,**kwargs):\n",
    "    projection = ccrs.Robinson(central_longitude=-135.5808361)\n",
    "    fig, ax = plt.subplots(\n",
    "        figsize=(8, 4), subplot_kw={\"projection\": projection}, constrained_layout=True\n",
    "    )\n",
    "    ax.set_global()\n",
    "    if extent is not None:\n",
    "        ax.set_extent(extent, crs=ccrs.PlateCarree())\n",
    "    egh.healpix_show(var, ax=ax, **kwargs)\n",
    "    if title is not None:\n",
    "        ax.set_title(title)\n",
    "    ax.add_feature(cf.COASTLINE, linewidth=0.8)\n",
    "    ax.add_feature(cf.BORDERS, linewidth=0.4)\n",
    "    \n",
    "def get_nn_lon_lat_index(nside, lons, lats):\n",
    "    lons2, lats2 = np.meshgrid(lons, lats)\n",
    "    return xr.DataArray(\n",
    "        hp.ang2pix(nside, lons2, lats2, nest=True, lonlat=True),\n",
    "        coords=[(\"lat\", lats), (\"lon\", lons)],\n",
    "    )\n",
    "\n",
    "def get_nside(ds):\n",
    "    return ds.crs.healpix_nside\n",
    "\n",
    "def get_nn_data(var, nx=1000, ny=1000, ax=None):\n",
    "    \"\"\"\n",
    "    var: variable (array-like)\n",
    "    nx: image resolution in x-direction\n",
    "    ny: image resolution in y-direction\n",
    "    ax: axis to plot on\n",
    "    returns: values on the points in the plot grid.\n",
    "    \"\"\"\n",
    "    lonlat = get_lonlat_for_plot_grid(nx, ny, ax)\n",
    "    try:\n",
    "        return get_healpix_nn_data(var, lonlat)\n",
    "    except ValueError:\n",
    "        pass\n",
    "    if set(var.dims) == {\"lat\", \"lon\"}:\n",
    "        return get_lonlat_meshgrid_nn_data(var, lonlat)\n",
    "    else:\n",
    "        return get_lonlat_nn_data(var, lonlat)\n",
    "\n",
    "\n",
    "def get_healpix_nn_data(var, lonlat):\n",
    "    \"\"\"\n",
    "    var: variable on healpix coordinates (array-like)\n",
    "    lonlat: coordinates at which to get the data\n",
    "    returns: values on the points in the plot grid.\n",
    "    \"\"\"\n",
    "    valid = np.all(np.isfinite(lonlat), axis=-1)\n",
    "    points = lonlat[valid].T  # .T reverts index order\n",
    "    pix = hp.ang2pix(\n",
    "        hp.npix2nside(len(var)), theta=points[0], phi=points[1], nest=True, lonlat=True\n",
    "    )\n",
    "    res = np.full(lonlat.shape[:-1], np.nan, dtype=var.dtype)\n",
    "    res[valid] = var[pix]\n",
    "    return res\n",
    "\n",
    "\n",
    "def get_lonlat_nn_data(var, lonlat):\n",
    "    \"\"\"\n",
    "    var: variable with lon and lat attributes (2d slice)\n",
    "    lonlat: coordinates at which to get the data\n",
    "    returns: values on the points in the plot grid.\n",
    "    \"\"\"\n",
    "    var_xyz = lonlat_to_xyz(lon=var.lon.values.flatten(), lat=var.lat.values.flatten())\n",
    "    tree = KDTree(var_xyz)\n",
    "\n",
    "    valid = np.all(np.isfinite(lonlat), axis=-1)\n",
    "    ll_valid = lonlat[valid].T\n",
    "    plot_xyz = lonlat_to_xyz(lon=ll_valid[0], lat=ll_valid[1])\n",
    "\n",
    "    distances, inds = tree.query(plot_xyz)\n",
    "    res = np.full(lonlat.shape[:-1], np.nan, dtype=var.dtype)\n",
    "    res[valid] = var.values.flatten()[inds]\n",
    "    return res\n",
    "\n",
    "\n",
    "def get_lonlat_meshgrid_nn_data(var, lonlat):\n",
    "    \"\"\"\n",
    "    var: variable with lon and lat attributes (2d slice)\n",
    "    lonlat: coordinates at which to get the data\n",
    "    returns: values on the points in the plot grid.\n",
    "    \"\"\"\n",
    "    return get_lonlat_nn_data(var.stack(cell=(\"lon\", \"lat\")), lonlat)\n",
    "\n",
    "\n",
    "def get_lonlat_for_plot_grid(nx, ny, ax=None):\n",
    "    \"\"\"\n",
    "    nx: image resolution in x-direction\n",
    "    ny: image resolution in y-direction\n",
    "    ax: axis to plot on\n",
    "    returns: coordinates of the points in the plot grid.\n",
    "    \"\"\"\n",
    "\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "\n",
    "    xlims = ax.get_xlim()\n",
    "    ylims = ax.get_ylim()\n",
    "    xvals = np.linspace(xlims[0], xlims[1], nx)\n",
    "    yvals = np.linspace(ylims[0], ylims[1], ny)\n",
    "    xvals2, yvals2 = np.meshgrid(xvals, yvals)\n",
    "    lonlat = ccrs.PlateCarree().transform_points(\n",
    "        ax.projection, xvals2, yvals2, np.zeros_like(xvals2)\n",
    "    )\n",
    "    return lonlat\n",
    "\n",
    "\n",
    "def lonlat_to_xyz(lon, lat):\n",
    "    \"\"\"\n",
    "    lon: longitude in degree E\n",
    "    lat: latitude in degree N\n",
    "    returns numpy array (3, len (lon)) with coordinates on unit sphere.\n",
    "    \"\"\"\n",
    "\n",
    "    return np.array(\n",
    "        (\n",
    "            np.cos(np.deg2rad(lon)) * np.cos(np.deg2rad(lat)),\n",
    "            np.sin(np.deg2rad(lon)) * np.cos(np.deg2rad(lat)),\n",
    "            np.sin(np.deg2rad(lat)),\n",
    "        )\n",
    "    ).T\n",
    "\n",
    "def plot_map_diff(var, ref, colorbar_label=\"\", title=\"\", extent=None, **kwargs):\n",
    "    \"\"\"\n",
    "    var: data set\n",
    "    ref: reference data\n",
    "    colorbar_label: label for the colorbar\n",
    "    title: title string\n",
    "    **kwargs: get passed to imshow\n",
    "    returns figure, axis objects\n",
    "    \"\"\"\n",
    "    projection = ccrs.Robinson(central_longitude=-135.5808361)\n",
    "    fig, ax = plt.subplots(\n",
    "        figsize=(8, 4), subplot_kw={\"projection\": projection}, constrained_layout=True\n",
    "    )\n",
    "    ax.set_global()\n",
    "    if extent is not None:\n",
    "        ax.set_extent(extent, crs=ccrs.PlateCarree())\n",
    "\n",
    "    varmap = get_nn_data(var, ax=ax)\n",
    "    refmap = get_nn_data(ref, ax=ax)\n",
    "    imsh = ax.imshow(\n",
    "        varmap - refmap, extent=ax.get_xlim() + ax.get_ylim(), origin=\"lower\", **kwargs\n",
    "    )\n",
    "\n",
    "    # Add coastlines and borders\n",
    "    ax.add_feature(cf.COASTLINE, linewidth=0.8)\n",
    "    ax.add_feature(cf.BORDERS, linewidth=0.4)\n",
    "\n",
    "    # âœ… Add gridlines\n",
    "    gl = ax.gridlines(draw_labels=True, linewidth=0.5, color='gray', alpha=0.5, linestyle='--')\n",
    "    gl.top_labels = False  # Disable labels on top\n",
    "    gl.right_labels = False  # Disable labels on right\n",
    "    gl.xlabel_style = {'size': 8}\n",
    "    gl.ylabel_style = {'size': 8}\n",
    "\n",
    "    # Colorbar and title\n",
    "    fig.colorbar(imsh, label=colorbar_label)\n",
    "    plt.title(title)\n",
    "    return (fig, ax)\n",
    "\n",
    "\n",
    "def compute_SST_wind_derivative_fields(str_a,str_b,sigma,llon,llat,sst,u_interp,v_interp):\n",
    "    # Get the background wind field.\n",
    "    smooth_u = gm.nan_gaussian_filter(u_interp,sigma)\n",
    "    smooth_v = gm.nan_gaussian_filter(v_interp,sigma)\n",
    "    smooth_ws = np.sqrt(smooth_u**2+smooth_v**2)\n",
    "    smooth_sst = gm.nan_gaussian_filter(sst,sigma)\n",
    "\n",
    "    # Large-scale winddir\n",
    "    cosphi = smooth_u/smooth_ws\n",
    "    sinphi = smooth_v/smooth_ws\n",
    "    \n",
    "    # Get the anomalies with respect to the background wind field.\n",
    "    u_prime = u_interp-smooth_u\n",
    "    v_prime = v_interp-smooth_v\n",
    "    sst_prime = sst-smooth_sst\n",
    "\n",
    "    dsst_dx, dsst_dy = gm.grad_sphere(sst_prime,llon,llat)\n",
    "    if str_a=='gamma':\n",
    "        a_prime = u_interp*dsst_dx + v_interp*dsst_dy\n",
    "    elif str_a=='dsst_dr':\n",
    "        a_prime = dsst_dx*cosphi + dsst_dy*sinphi\n",
    "    elif str_a=='lapl_sst':\n",
    "        a_prime = gm.div_sphere(dsst_dx,dsst_dy,llon,llat)\n",
    "    elif str_a=='d2sst_ds2':\n",
    "        dsst_ds = -dsst_dx*sinphi + dsst_dy*cosphi\n",
    "        ddsst_ds_dx, ddsst_ds_dy = gm.grad_sphere(dsst_ds,llon,llat)\n",
    "        a_prime = -ddsst_ds_dx*sinphi + ddsst_ds_dy*cosphi\n",
    "    elif str_a=='sst_prime':\n",
    "        smooth_sst = gm.nan_gaussian_filter(l3_sst,sigma)\n",
    "        a_prime = l3_sst-smooth_sst\n",
    "\t\t\n",
    "    if str_b=='wind_div':\n",
    "        b_prime = gm.div_sphere(u_interp,v_interp,llon,llat)\n",
    "    elif str_b=='dr_dot_prime_dr':\n",
    "        r_dot_prime = u_prime*cosphi + v_prime*sinphi\n",
    "        dr_dot_prime_dx, dr_dot_prime_dy = gm.grad_sphere(r_dot_prime,llon,llat)\n",
    "        b_prime = dr_dot_prime_dx*cosphi + dr_dot_prime_dy*sinphi \n",
    "    elif str_b=='ds_dot_prime_ds':\n",
    "        s_dot_prime = -u_prime*sinphi + v_prime*cosphi\n",
    "        ds_dot_prime_dx, ds_dot_prime_dy = gm.grad_sphere(s_dot_prime,llon,llat)\n",
    "        b_prime = -ds_dot_prime_dx*sinphi + ds_dot_prime_dy*cosphi\n",
    "    elif str_b=='ws_prime':\n",
    "        b_prime = np.sqrt(u_interp**2+v_interp**2)-smooth_ws\n",
    "\n",
    "\n",
    "    # Remove the NaNs, from the variables to be concatenated (with no subsampling).\n",
    "    #a_to_be_concat = a_prime[(~np.isnan(a_prime))&(~np.isnan(b_prime))&(~np.isnan(smooth_ws))]\n",
    "    #b_to_be_concat = b_prime[(~np.isnan(a_prime))&(~np.isnan(b_prime))&(~np.isnan(smooth_ws))]    \n",
    "    #U_to_be_concat = smooth_ws[(~np.isnan(a_prime))&(~np.isnan(b_prime))&(~np.isnan(smooth_ws))]    \n",
    "\n",
    "    return a_prime, b_prime, smooth_ws #a_to_be_concat, b_to_be_concat, U_to_be_concat\n",
    "\n",
    "\n",
    "def compute_SST_var_derivative_fields(str_a,str_b,sigma,llon,llat,sst,u_interp,v_interp,var):\n",
    "    # Get the background wind field.\n",
    "    smooth_u = gm.nan_gaussian_filter(u_interp,sigma)\n",
    "    smooth_v = gm.nan_gaussian_filter(v_interp,sigma)\n",
    "    smooth_ws = np.sqrt(smooth_u**2+smooth_v**2)\n",
    "    smooth_sst = gm.nan_gaussian_filter(sst,sigma)\n",
    "    smooth_var = gm.nan_gaussian_filter(var,sigma)\n",
    "\n",
    "    # Large-scale winddir\n",
    "    cosphi = smooth_u/smooth_ws\n",
    "    sinphi = smooth_v/smooth_ws\n",
    "    \n",
    "    # Get the anomalies with respect to the background wind field.\n",
    "    u_prime = u_interp-smooth_u\n",
    "    v_prime = v_interp-smooth_v\n",
    "    sst_prime = sst-smooth_sst\n",
    "    var_prime = var-smooth_var\n",
    "\n",
    "    dsst_dx, dsst_dy = gm.grad_sphere(sst_prime,llon,llat)\n",
    "    if str_a=='gamma':\n",
    "        a_prime = u_interp*dsst_dx + v_interp*dsst_dy\n",
    "    elif str_a=='dsst_dr':\n",
    "        a_prime = dsst_dx*cosphi + dsst_dy*sinphi\n",
    "    elif str_a=='lapl_sst':\n",
    "        a_prime = gm.div_sphere(dsst_dx,dsst_dy,llon,llat)\n",
    "    elif str_a=='d2sst_ds2':\n",
    "        dsst_ds = -dsst_dx*sinphi + dsst_dy*cosphi\n",
    "        ddsst_ds_dx, ddsst_ds_dy = gm.grad_sphere(dsst_ds,llon,llat)\n",
    "        a_prime = -ddsst_ds_dx*sinphi + ddsst_ds_dy*cosphi\n",
    "    elif str_a=='sst_prime':\n",
    "        smooth_sst = gm.nan_gaussian_filter(l3_sst,sigma)\n",
    "        a_prime = l3_sst-smooth_sst\n",
    "\n",
    "    dvar_dx, dvar_dy = gm.grad_sphere(var_prime,llon,llat)\n",
    "    if str_b=='wind_div':\n",
    "        b_prime = gm.div_sphere(u_interp,v_interp,llon,llat)\n",
    "    elif str_b=='dr_dot_prime_dr':\n",
    "        b_prime = dvar_dx*cosphi + dvar_dy*sinphi\n",
    "    elif str_b=='ds_dot_prime_ds':\n",
    "        b_prime = -dvar_dx*sinphi + dvar_dy*cosphi\n",
    "    elif str_b=='ws_prime':\n",
    "        b_prime = np.sqrt(u_interp**2+v_interp**2)-smooth_ws\n",
    "\n",
    "\n",
    "    # Remove the NaNs, from the variables to be concatenated (with no subsampling).\n",
    "    #a_to_be_concat = a_prime[(~np.isnan(a_prime))&(~np.isnan(b_prime))&(~np.isnan(smooth_ws))]\n",
    "    #b_to_be_concat = b_prime[(~np.isnan(a_prime))&(~np.isnan(b_prime))&(~np.isnan(smooth_ws))]    \n",
    "    #U_to_be_concat = smooth_ws[(~np.isnan(a_prime))&(~np.isnan(b_prime))&(~np.isnan(smooth_ws))]    \n",
    "\n",
    "    return a_prime, b_prime, smooth_ws #a_to_be_concat, b_to_be_concat, U_to_be_concat\n",
    "\n",
    "\n",
    "\n",
    "def binned_mean_plot(x, y, bins=10, xlabel='x', ylabel='Mean y',title=None):\n",
    "    \"\"\"\n",
    "    Bin x into equally spaced intervals and compute mean y in each bin.\n",
    "    \n",
    "    Parameters:\n",
    "    - x: array-like, independent variable\n",
    "    - y: array-like, dependent variable\n",
    "    - bins: int, number of bins\n",
    "    - xlabel: str, label for x-axis\n",
    "    - ylabel: str, label for y-axis\n",
    "    \"\"\"\n",
    "    x = np.asarray(x)\n",
    "    y = np.asarray(y)\n",
    "\n",
    "    # Compute bin edges and indices\n",
    "    bin_edges = np.linspace(np.nanmin(x), np.nanmax(x), bins + 1)\n",
    "    bin_indices = np.digitize(x, bin_edges) - 1  # bins are 0-indexed\n",
    "\n",
    "    bin_centers = 0.5 * (bin_edges[:-1] + bin_edges[1:])\n",
    "    mean_y = np.array([\n",
    "        np.nanmean(y[bin_indices == i]) if np.any(bin_indices == i) else np.nan\n",
    "        for i in range(bins)\n",
    "    ])\n",
    "    std_y = np.array([\n",
    "        np.nanstd(y[bin_indices == i]) if np.any(bin_indices == i) else np.nan\n",
    "        for i in range(bins)\n",
    "    ])\n",
    "\n",
    "    lin_regr = binned_weighted_lin_regr(a_prime, b_prime, bins=10)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.errorbar(x=bin_centers,y=mean_y,yerr=std_y,marker='o', linestyle='-', color='black')\n",
    "    plt.plot(bin_centers,lin_regr['intercept'][0]+bin_centers*lin_regr['slope'][0],marker='', linestyle='-', color='red')\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def weighted_linear_regression_with_pvalues(x, y, sigma_y):\n",
    "    '''         \n",
    "        Compute a weighted linear regressio of (x,y+/-sigma_y),\n",
    "        where the uncertainty on y observations is taken into \n",
    "        account.\n",
    "        Returns a dictionary in the form:\n",
    "                \"intercept\" : (intercept, sigma_b),\n",
    "                \"slope\" : (slope, sigma_m),\n",
    "                \"chi2\" : chi square value,\n",
    "                \"q\" : goodness of fit\n",
    "\n",
    "        From Numerical Recipe 15.2\n",
    "        If q is larger than, say, 0.1,then the goodness-of-fit is believable. \n",
    "        If it is larger than, say, 0.001, then the fit may be acceptable if the \n",
    "        errors are nonnormal or have been moderately underestimated. If q is \n",
    "        less than 0.001, then the model and/or estimation procedure can rightly\n",
    "        be called into question. In this latter case, turn to 15.7 to proceed \n",
    "        further.\n",
    "    '''\n",
    "\n",
    "    # Numerical recipe code 15.2 (https://numerical.recipes/book.html)\n",
    "    # Translated to python by copilot and checked.\n",
    "    ndata = len(x)\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    sigma_y = np.array(sigma_y)\n",
    "    \n",
    "    ss = 0.0\n",
    "    sx = 0.0\n",
    "    sy = 0.0\n",
    "    st2 = 0.0\n",
    "    b = 0.0\n",
    "    chi2 = 0.0\n",
    "    q = 1.0 # Estimator for goodness of fit.\n",
    "    \n",
    "    '''\n",
    "        From Numerical Recipe 15.2\n",
    "        If Q is larger than, say, 0.1,then the goodness-of-fit is believable. \n",
    "        If it is larger than, say, 0.001, then the fit may be acceptable if the \n",
    "        errors are nonnormal or have been moderately underestimated. If Q is \n",
    "        less than 0.001, then the model and/or estimation procedure can rightly\n",
    "        be called into question. In this latter case, turn to 15.7 to proceed \n",
    "        further.\n",
    "    '''\n",
    "\n",
    "    for i in range(ndata):\n",
    "        wt = 1.0 / (sigma_y[i] ** 2)\n",
    "        ss += wt\n",
    "        sx += x[i] * wt\n",
    "        sy += y[i] * wt\n",
    "\n",
    "    sxoss = sx / ss\n",
    "\n",
    "    for i in range(ndata):\n",
    "        t = (x[i] - sxoss) / sigma_y[i]\n",
    "        st2 += t * t\n",
    "        b += t * y[i] / sigma_y[i]\n",
    "\n",
    "    b /= st2\n",
    "    a = (sy - sx * b) / ss\n",
    "    sigma_a = np.sqrt((1.0 + sx * sx / (ss * st2)) / ss)\n",
    "    sigma_b = np.sqrt(1.0 / st2)\n",
    "\n",
    "    for i in range(ndata):\n",
    "        chi2 += ((y[i] - a - b * x[i]) / sigma_y[i]) ** 2\n",
    "\n",
    "    if ndata > 2:\n",
    "        q = gammaincc(0.5 * (ndata - 2), 0.5 * chi2)\n",
    "\n",
    "    return {\n",
    "        \"intercept\": (a, sigma_a),\n",
    "        \"slope\": (b, sigma_b),\n",
    "        \"chi2\" : chi2,\n",
    "        \"q\" : q,\n",
    "    }\n",
    "\n",
    "def binned_weighted_lin_regr(x, y, bins=10):\n",
    "    \"\"\"\n",
    "    Bin x into equally spaced intervals and compute the lin regr y(x) considering std(y) for each bin.\n",
    "    \n",
    "    Parameters:\n",
    "    - x: array-like, independent variable\n",
    "    - y: array-like, dependent variable\n",
    "    - bins: int, number of bins\n",
    "    \"\"\"\n",
    "    x = np.asarray(x)\n",
    "    y = np.asarray(y)\n",
    "\n",
    "    # Compute bin edges and indices\n",
    "    bin_edges = np.linspace(np.nanmin(x), np.nanmax(x), bins + 1)\n",
    "    bin_indices = np.digitize(x, bin_edges) - 1  # bins are 0-indexed\n",
    "\n",
    "    bin_centers = 0.5 * (bin_edges[:-1] + bin_edges[1:])\n",
    "    mean_y = np.array([\n",
    "        np.nanmean(y[bin_indices == i]) if np.any(bin_indices == i) else np.nan\n",
    "        for i in range(bins)\n",
    "    ])\n",
    "    std_y = np.array([\n",
    "        np.nanstd(y[bin_indices == i]) if np.any(bin_indices == i) else np.nan\n",
    "        for i in range(bins)\n",
    "    ])\n",
    "\n",
    "    bin_centers,mean_y,std_y = bin_centers[np.isfinite(mean_y) & np.isfinite(std_y) & (std_y != 0)],mean_y[np.isfinite(mean_y) & np.isfinite(std_y) & (std_y != 0)],std_y[np.isfinite(mean_y) & np.isfinite(std_y) & (std_y != 0)]\n",
    "\n",
    "    return weighted_linear_regression_with_pvalues(bin_centers, mean_y, std_y)\n",
    "    \n",
    "\n",
    "def plot_map_with_contours(data,llat,llon,background=None,title=None,cmap=None,vmin=None,vmax=None,label=None, contour_levels=10):\n",
    "    \"\"\"\n",
    "    Plot a 2D data array on a map with optional background contours.\n",
    "\n",
    "    Parameters:\n",
    "    - data: 2D array (lat x lon)\n",
    "    - llat, llon: 2D arrays matching shape of data\n",
    "    - background: Optional 2D array (same shape) to use as contour overlay\n",
    "    - title: Plot title\n",
    "    - cmap: Colormap for main data\n",
    "    - contour_levels: Number or list of contour levels\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(10, 6))\n",
    "    ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "\n",
    "    # Main data plot\n",
    "    mesh = ax.pcolormesh(llon, llat, data, transform=ccrs.PlateCarree(), cmap=cmap,vmin=vmin,vmax=vmax)\n",
    "\n",
    "    # Add optional contours\n",
    "    if background is not None:\n",
    "        contours = ax.contour(llon, llat, background, levels=contour_levels, colors='black',\n",
    "                              linewidths=0.6, transform=ccrs.PlateCarree())\n",
    "        ax.clabel(contours, inline=True, fontsize=8)\n",
    "\n",
    "    ax.coastlines()\n",
    "    ax.add_feature(cf.BORDERS, linewidth=0.5)\n",
    "    ax.gridlines(draw_labels=True, linewidth=0.5, linestyle='--')\n",
    "    plt.colorbar(mesh, ax=ax, orientation='vertical', label=label,shrink=0.5)\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def runCOARE(ds, uas, vas, psl, sst, tas, hur, pr, rsds, rlds, lat_name='lat', lon_name='lon'):\n",
    "    ''' COARE requires the following input atmospheric variables:\n",
    "        source: https://github.com/pyCOARE/coare/blob/main/pycoare/coare_35.py\n",
    "\n",
    "        u: ocean surface wind speed (m/s) at height zu\n",
    "        t: bulk air temperature (degC) at height zt\n",
    "        rh: relative humidity (%) at height zq\n",
    "        ts: sea water temperature (degC) (also see jcool)\n",
    "        p: surface air pressure (mb)\n",
    "        zi: planetary boundary layer height (m)\n",
    "        rs: downward shortwave radiation (W/m^2)\n",
    "        rl: downward longwave radiation (W/m^2)\n",
    "        rain: rain rate (mm/hr)\n",
    "\n",
    "        Note: ocean variables and sensor heights ignored and set to COARE default for now.\n",
    "\n",
    "        Inputs to this wrapping function:\n",
    "            - IFS dataset with all atmospheric variables\n",
    "            - Each atmospheric variable as: ds[\"var\"]\n",
    "            - Names of your lat/lon coordinates (if different from standard)\n",
    "\n",
    "        Output: Dataset inputted into function with processed atmospheric variables\n",
    "                & variable of interest from COARE (in this case wind stress).\n",
    "                See source for other COARE variables (good luck, COARE code is a handful).\n",
    "\n",
    "    '''\n",
    "    # define shape of grid\n",
    "    dimensions = list(ds.dims)\n",
    "    grid_shape = tas.shape\n",
    "\n",
    "    # compute wind speed from wind vectors\n",
    "    ds['wsp'] = ((lat_name, lon_name), np.sqrt(uas**2 + vas**2).data)\n",
    "\n",
    "    # express pressure as hPa instead of Pa\n",
    "    ds['psl'] = ds['psl'] * 0.01\n",
    "\n",
    "    # express sst and temp air as deg C not K\n",
    "    ds['sst'] = ds['sst']- 273.15\n",
    "    ds['tas'] = ds['tas'] - 273.15\n",
    "\n",
    "    # express relative humidity in %\n",
    "    ds['hur'] = ds['hur'] * 100\n",
    "\n",
    "    # convert rain rate from kg/m2/s to mm/hr\n",
    "    ds['pr'] = ds['pr'] * 3600\n",
    "\n",
    "    # run COARE\n",
    "    bulk_params = coare_35(u = ds[\"wsp\"].data.flatten(), t = ds[\"tas\"].data.flatten(),\n",
    "                            rh = ds[\"hur\"].data.flatten(), ts = ds[\"sst\"].data.flatten(),\n",
    "                            p = ds[\"psl\"].data.flatten(), rs = ds[\"rsds\"].data.flatten(),\n",
    "                            rl = ds[\"rlds\"].data.flatten(), rain = ds[\"pr\"].data.flatten())\n",
    "\n",
    "    # retrieve variable of interest (in this case wind stress)\n",
    "    ds['cwst'] = (dimensions, bulk_params._return_vars('tau').reshape(grid_shape))\n",
    "    ds['ccdrag'] = (dimensions, bulk_params._return_vars('cd').reshape(grid_shape))\n",
    "    ds['ccdragn'] = (dimensions, bulk_params._return_vars('cdn_rf').reshape(grid_shape))\n",
    "\n",
    "    return ds\n",
    "\n",
    "def save_friday_data(ds,friday):\n",
    "    print(f'{friday} # Start')\n",
    "    fine_ds = ds.sel(time=friday)\n",
    "    fine_ds = fine_ds.drop_vars(['lat','lon'])\n",
    "    for var in fine_ds.data_vars:\n",
    "        if np.issubdtype(fine_ds[var].dtype, np.floating):\n",
    "            fine_ds[var] = fine_ds[var].where(fine_ds[var] != 9999, np.nan)\n",
    "    # Regrid and select region\n",
    "    region_idx = get_nn_lon_lat_index(2**fine_zoom, np.arange(regions[region]['boundaries'][0], regions[region]['boundaries'][1], fine_latlon_gridstep/supersampling['lon']), np.arange(regions[region]['boundaries'][2], regions[region]['boundaries'][3], fine_latlon_gridstep/supersampling['lat']))\n",
    "    \n",
    "    print(f'{friday} # Compute dask')\n",
    "    fine_sst = fine_ds.sst.where(fine_ds.sst!=9999).isel(cell=region_idx).coarsen(supersampling).mean().compute()\n",
    "    fine_tas = fine_ds['tas'].isel(cell=region_idx).coarsen(supersampling).mean().compute()\n",
    "    fine_10u = fine_ds['uas'].isel(cell=region_idx).coarsen(supersampling).mean().compute()\n",
    "    fine_10v = fine_ds['vas'].isel(cell=region_idx).coarsen(supersampling).mean().compute()\n",
    "    fine_blh = fine_ds['blh'].isel(cell=region_idx).coarsen(supersampling).mean().compute()\n",
    "    # Remove land\n",
    "    fine_sst = fine_sst.where(np.isfinite(fine_sst))\n",
    "    fine_10u = fine_10u.where(np.isfinite(fine_sst))\n",
    "    fine_10v = fine_10v.where(np.isfinite(fine_sst))\n",
    "    fine_blh = fine_blh.where(np.isfinite(fine_sst))\n",
    "    \n",
    "    llon, llat = np.meshgrid(fine_sst.lon,fine_sst.lat)\n",
    "\n",
    "    # SST grad, wind and blh div along large-scale wind direction\n",
    "    # Sub-mesoscale\n",
    "    print(f'{friday} # Compute fields')\n",
    "    sst_prime, wind_prime, smooth_ws = compute_SST_wind_derivative_fields(sst_deriv,wind_deriv,submeso_pass_lower_sigma,llon,llat,fine_sst,fine_10u,fine_10v)\n",
    "    _, bhl_prime, _ = compute_SST_var_derivative_fields(sst_deriv,wind_deriv,submeso_pass_lower_sigma,llon,llat,fine_sst,fine_10u,fine_10v,fine_blh)\n",
    "\n",
    "    smooth_tas = gm.nan_gaussian_filter(fine_tas,submeso_pass_lower_sigma)\n",
    "    smooth_sst = gm.nan_gaussian_filter(fine_sst,submeso_pass_lower_sigma)\n",
    "\n",
    "    output = {\n",
    "        sst_deriv : sst_prime.flatten(),\n",
    "        wind_deriv : wind_prime.flatten(),\n",
    "        blh_deriv : bhl_prime.flatten(),\n",
    "        'smooth_ws' : smooth_ws.flatten(),\n",
    "        'smooth_tas' : smooth_tas.flatten(),\n",
    "        'smooth_sst' : smooth_sst.flatten(),\n",
    "    }\n",
    "    \n",
    "    output_pd = pd.DataFrame.from_dict(output)\n",
    "    \n",
    "    if enable_save_file:\n",
    "        output_pd.to_csv(f'{WORKDIR}/fields/{str_mech}_zoom_{fine_zoom}_timeres_{time_res}_latlon_res_{fine_latlon_gridstep*100}km_high_pass_{submeso_pass_lower_sigma}_{region}_{friday}.csv')\n",
    "    print(f'{friday} # DONE')\n",
    "\n",
    "def parallel_apply(my_function,arr):\n",
    "    with ProcessPoolExecutor() as executor:\n",
    "        results = list(executor.map(my_function, arr))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbaf69c-916e-4baa-91d8-6f0d2cc45bfc",
   "metadata": {},
   "source": [
    "# Data extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff8a246-7f28-418b-b776-bb2379684a78",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d8d3066-5260-4fbf-8746-270aeda5ac2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define configuration\n",
    "model='ifs_tco3999-ng5_rcbmf_cf'\n",
    "time_res = 'PT1H'\n",
    "fine_zoom = 11\n",
    "fine_latlon_gridstep = 0.02 # [Â°]\n",
    "supersampling = {\"lon\": 4, \"lat\": 4}\n",
    "submeso_pass_lower_sigma = 5 # [gridstep], for ifs_zoom = 11 => 0.01Â° latlon resampling => 10km gaussian smoothing => ~20km filter \n",
    "\n",
    "#region = 'eurec4a_extended'\n",
    "region = 'gulf_stream'\n",
    "#region = 'EAC'\n",
    "regions = {\n",
    "    'gulf_stream' : {\n",
    "        'long_name' : 'Gulf Stream',\n",
    "        'short_name' : 'gulf_stream',\n",
    "        'boundaries' : [-83,-30,30,55], # [minlon,maxlon,minlat,maxlat]. lon -> [-180,180], lat -> [-90,90]  \n",
    "    },    \n",
    "    'gulf_stream_detail' : {\n",
    "        'long_name' : 'Gulf Stream - detail',\n",
    "        'short_name' : 'gulf_stream_detail',\n",
    "        'boundaries' : [-73,-50,30,45], # [minlon,maxlon,minlat,maxlat]. lon -> [-180,180], lat -> [-90,90]  \n",
    "    },    \n",
    "    'tropical_atlantic' : {\n",
    "        'long_name' : 'Tropical Atlantic',\n",
    "        'short_name' : 'tropical_atlantic',\n",
    "        'boundaries' : [-62,15,-20,20], # [minlon,maxlon,minlat,maxlat]. lon -> [-180,180], lat -> [-90,90]  \n",
    "    },\n",
    "    'tropical_atlantic_detail' : {\n",
    "        'long_name' : 'Tropical Atlantic - detail',\n",
    "        'short_name' : 'tropical_atlantic_detail',\n",
    "        'boundaries' : [-40,-25,-5,0], # [minlon,maxlon,minlat,maxlat]. lon -> [-180,180], lat -> [-90,90]  \n",
    "    },\n",
    "    'eurec4a' : {\n",
    "        'long_name' : '$EUREC^{4}A$',\n",
    "        'short_name' : 'eurec4a',\n",
    "        'boundaries' : [-62,-48,4,16], # [minlon,maxlon,minlat,maxlat]. lon -> [-180,180], lat -> [-90,90]  \n",
    "    },\n",
    "    'eurec4a_extended' : {\n",
    "        'long_name' : r'extended $EUREC^{4}A$',\n",
    "        'short_name' : 'eurec4a_extended',\n",
    "        'boundaries' : [-62,-20,0,20], # [minlon,maxlon,minlat,maxlat]. lon -> [-180,180], lat -> [-90,90]  \n",
    "    },\n",
    "    'EAC' : {\n",
    "        'long_name' : r'East Australian Current',\n",
    "        'short_name' : 'EAC',\n",
    "        'boundaries' : [150,165,-45,-25], # [minlon,maxlon,minlat,maxlat]. lon -> [-180,180], lat -> [-90,90]  \n",
    "    },\n",
    "}\n",
    "\n",
    "# Select here the fields to be analysed.\n",
    "str_mech = 'DMM'\n",
    "#str_mech = 'PA'\n",
    "if str_mech == 'DMM':\n",
    "    sst_deriv = 'dsst_dr' # Choose between: 'dsst_dr', 'lapl_sst', 'd2sst_ds2', 'sst_prime'\n",
    "    wind_deriv = 'dr_dot_prime_dr' # Choose between: 'wind_div', 'dr_dot_prime_dr', 'ds_dot_prime_ds', 'ws_prime'\n",
    "    blh_deriv = 'dblh_prime_dr'\n",
    "elif str_mech == 'PA':\n",
    "    sst_deriv = 'd2sst_ds2'\n",
    "    wind_deriv = 'ds_dot_prime_ds'\n",
    "    wind_deriv = 'dblh_prime_ds'\n",
    "else: \n",
    "    raise NameError('Mechanism not recognised')\n",
    "\n",
    "WORKDIR = '/work/bb1153/b383497' \n",
    "enable_save_file = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2620a8-2352-41a8-ac64-0d449d72c9e8",
   "metadata": {},
   "source": [
    "## Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc1cdbb9-3649-4c7f-b536-aa3372fb0aa9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   name                     description type  allowed default\n",
      "0  time  time resolution of the dataset  str   [PT1H]    PT1H\n",
      "1  zoom       zoom level of the dataset  int  [7, 11]       7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['CERES_EBAF',\n",
       " 'ERA5',\n",
       " 'IR_IMERG',\n",
       " 'JRA3Q',\n",
       " 'MERRA2',\n",
       " 'arp-gem-1p3km',\n",
       " 'arp-gem-2p6km',\n",
       " 'casesm2_10km_nocumulus',\n",
       " 'icon_d3hp003',\n",
       " 'icon_d3hp003aug',\n",
       " 'icon_d3hp003feb',\n",
       " 'icon_ngc4008',\n",
       " 'ifs_tco2559_rcbmf',\n",
       " 'ifs_tco3999-ng5_deepoff',\n",
       " 'ifs_tco3999-ng5_rcbmf',\n",
       " 'ifs_tco3999-ng5_rcbmf_cf',\n",
       " 'ifs_tco3999_rcbmf',\n",
       " 'nicam_220m_test',\n",
       " 'nicam_gl11',\n",
       " 'scream-dkrz',\n",
       " 'tracking-d3hp003',\n",
       " 'um_Africa_km4p4_RAL3P3_n1280_GAL9_nest',\n",
       " 'um_CTC_km4p4_RAL3P3_n1280_GAL9_nest',\n",
       " 'um_SAmer_km4p4_RAL3P3_n1280_GAL9_nest',\n",
       " 'um_SEA_km4p4_RAL3P3_n1280_GAL9_nest',\n",
       " 'um_glm_n1280_CoMA9_TBv1p2',\n",
       " 'um_glm_n1280_GAL9',\n",
       " 'um_glm_n2560_RAL3p3']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat = intake.open_catalog(\"https://digital-earths-global-hackathon.github.io/catalog/catalog.yaml\")[\"EU\"]\n",
    "print(pd.DataFrame(cat[model].describe()[\"user_parameters\"]))\n",
    "list(cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ba0cc2-afaa-4050-a845-3f7a34cfc5b6",
   "metadata": {},
   "source": [
    "## Generate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "752d56c9-7dbd-4409-aa6f-3abe72536485",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b/b383497/.conda/envs/easy/lib/python3.12/site-packages/intake_xarray/xzarr.py:46: UserWarning: The specified chunks separate the stored chunks along dimension \"cell\" starting at index 33554432. This could degrade performance. Instead, consider rechunking after loading.\n",
      "  self._ds = xr.open_dataset(self.urlpath, **kw)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-02-07 13:00:48 # Start\n",
      "2020-02-07 13:00:48 # Compute dask\n",
      "2020-02-07 13:00:48 # Compute fields\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: nan_treatment='interpolate', however, NaN values detected post convolution. A contiguous region of NaN values, larger than the kernel size, are present in the input array. Increase the kernel size to avoid this. [astropy.convolution.convolve]\n",
      "WARNING: nan_treatment='interpolate', however, NaN values detected post convolution. A contiguous region of NaN values, larger than the kernel size, are present in the input array. Increase the kernel size to avoid this. [astropy.convolution.convolve]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-02-07 13:00:48 # DONE\n",
      "2020-02-07 14:00:32 # Start\n",
      "2020-02-07 14:00:32 # Compute dask\n",
      "2020-02-07 14:00:32 # Compute fields\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: nan_treatment='interpolate', however, NaN values detected post convolution. A contiguous region of NaN values, larger than the kernel size, are present in the input array. Increase the kernel size to avoid this. [astropy.convolution.convolve]\n",
      "WARNING: nan_treatment='interpolate', however, NaN values detected post convolution. A contiguous region of NaN values, larger than the kernel size, are present in the input array. Increase the kernel size to avoid this. [astropy.convolution.convolve]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-02-07 14:00:32 # DONE\n",
      "2020-02-07 15:00:16 # Start\n",
      "2020-02-07 15:00:16 # Compute dask\n",
      "2020-02-07 15:00:16 # Compute fields\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: nan_treatment='interpolate', however, NaN values detected post convolution. A contiguous region of NaN values, larger than the kernel size, are present in the input array. Increase the kernel size to avoid this. [astropy.convolution.convolve]\n",
      "WARNING: nan_treatment='interpolate', however, NaN values detected post convolution. A contiguous region of NaN values, larger than the kernel size, are present in the input array. Increase the kernel size to avoid this. [astropy.convolution.convolve]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-02-07 15:00:16 # DONE\n",
      "2020-02-07 16:00:00 # Start\n",
      "2020-02-07 16:00:00 # Compute dask\n",
      "2020-02-07 16:00:00 # Compute fields\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: nan_treatment='interpolate', however, NaN values detected post convolution. A contiguous region of NaN values, larger than the kernel size, are present in the input array. Increase the kernel size to avoid this. [astropy.convolution.convolve]\n",
      "WARNING: nan_treatment='interpolate', however, NaN values detected post convolution. A contiguous region of NaN values, larger than the kernel size, are present in the input array. Increase the kernel size to avoid this. [astropy.convolution.convolve]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-02-07 16:00:00 # DONE\n",
      "2020-02-07 16:59:44 # Start\n",
      "2020-02-07 16:59:44 # Compute dask\n",
      "2020-02-07 16:59:44 # Compute fields\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: nan_treatment='interpolate', however, NaN values detected post convolution. A contiguous region of NaN values, larger than the kernel size, are present in the input array. Increase the kernel size to avoid this. [astropy.convolution.convolve]\n",
      "WARNING: nan_treatment='interpolate', however, NaN values detected post convolution. A contiguous region of NaN values, larger than the kernel size, are present in the input array. Increase the kernel size to avoid this. [astropy.convolution.convolve]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-02-07 16:59:44 # DONE\n",
      "2020-02-07 17:59:28 # Start\n",
      "2020-02-07 17:59:28 # Compute dask\n",
      "2020-02-07 17:59:28 # Compute fields\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: nan_treatment='interpolate', however, NaN values detected post convolution. A contiguous region of NaN values, larger than the kernel size, are present in the input array. Increase the kernel size to avoid this. [astropy.convolution.convolve]\n",
      "WARNING: nan_treatment='interpolate', however, NaN values detected post convolution. A contiguous region of NaN values, larger than the kernel size, are present in the input array. Increase the kernel size to avoid this. [astropy.convolution.convolve]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-02-07 17:59:28 # DONE\n",
      "2020-02-07 18:59:12 # Start\n",
      "2020-02-07 18:59:12 # Compute dask\n",
      "2020-02-07 18:59:12 # Compute fields\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: nan_treatment='interpolate', however, NaN values detected post convolution. A contiguous region of NaN values, larger than the kernel size, are present in the input array. Increase the kernel size to avoid this. [astropy.convolution.convolve]\n",
      "WARNING: nan_treatment='interpolate', however, NaN values detected post convolution. A contiguous region of NaN values, larger than the kernel size, are present in the input array. Increase the kernel size to avoid this. [astropy.convolution.convolve]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-02-07 18:59:12 # DONE\n",
      "2020-02-07 20:01:04 # Start\n",
      "2020-02-07 20:01:04 # Compute dask\n",
      "2020-02-07 20:01:04 # Compute fields\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: nan_treatment='interpolate', however, NaN values detected post convolution. A contiguous region of NaN values, larger than the kernel size, are present in the input array. Increase the kernel size to avoid this. [astropy.convolution.convolve]\n",
      "WARNING: nan_treatment='interpolate', however, NaN values detected post convolution. A contiguous region of NaN values, larger than the kernel size, are present in the input array. Increase the kernel size to avoid this. [astropy.convolution.convolve]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-02-07 20:01:04 # DONE\n",
      "2020-02-07 21:00:48 # Start\n",
      "2020-02-07 21:00:48 # Compute dask\n",
      "2020-02-07 21:00:48 # Compute fields\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: nan_treatment='interpolate', however, NaN values detected post convolution. A contiguous region of NaN values, larger than the kernel size, are present in the input array. Increase the kernel size to avoid this. [astropy.convolution.convolve]\n",
      "WARNING: nan_treatment='interpolate', however, NaN values detected post convolution. A contiguous region of NaN values, larger than the kernel size, are present in the input array. Increase the kernel size to avoid this. [astropy.convolution.convolve]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-02-07 21:00:48 # DONE\n",
      "2020-02-07 22:00:32 # Start\n",
      "2020-02-07 22:00:32 # Compute dask\n",
      "2020-02-07 22:00:32 # Compute fields\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: nan_treatment='interpolate', however, NaN values detected post convolution. A contiguous region of NaN values, larger than the kernel size, are present in the input array. Increase the kernel size to avoid this. [astropy.convolution.convolve]\n",
      "WARNING: nan_treatment='interpolate', however, NaN values detected post convolution. A contiguous region of NaN values, larger than the kernel size, are present in the input array. Increase the kernel size to avoid this. [astropy.convolution.convolve]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-02-07 22:00:32 # DONE\n",
      "2020-02-07 23:00:16 # Start\n",
      "2020-02-07 23:00:16 # Compute dask\n",
      "2020-02-07 23:00:16 # Compute fields\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: nan_treatment='interpolate', however, NaN values detected post convolution. A contiguous region of NaN values, larger than the kernel size, are present in the input array. Increase the kernel size to avoid this. [astropy.convolution.convolve]\n",
      "WARNING: nan_treatment='interpolate', however, NaN values detected post convolution. A contiguous region of NaN values, larger than the kernel size, are present in the input array. Increase the kernel size to avoid this. [astropy.convolution.convolve]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-02-07 23:00:16 # DONE\n",
      "2020-02-14 00:00:00 # Start\n",
      "2020-02-14 00:00:00 # Compute dask\n",
      "2020-02-14 00:00:00 # Compute fields\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: nan_treatment='interpolate', however, NaN values detected post convolution. A contiguous region of NaN values, larger than the kernel size, are present in the input array. Increase the kernel size to avoid this. [astropy.convolution.convolve]\n",
      "WARNING: nan_treatment='interpolate', however, NaN values detected post convolution. A contiguous region of NaN values, larger than the kernel size, are present in the input array. Increase the kernel size to avoid this. [astropy.convolution.convolve]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-02-14 00:00:00 # DONE\n",
      "2020-02-14 00:59:44 # Start\n",
      "2020-02-14 00:59:44 # Compute dask\n",
      "2020-02-14 00:59:44 # Compute fields\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: nan_treatment='interpolate', however, NaN values detected post convolution. A contiguous region of NaN values, larger than the kernel size, are present in the input array. Increase the kernel size to avoid this. [astropy.convolution.convolve]\n",
      "WARNING: nan_treatment='interpolate', however, NaN values detected post convolution. A contiguous region of NaN values, larger than the kernel size, are present in the input array. Increase the kernel size to avoid this. [astropy.convolution.convolve]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-02-14 00:59:44 # DONE\n",
      "2020-02-14 01:59:28 # Start\n",
      "2020-02-14 01:59:28 # Compute dask\n",
      "2020-02-14 01:59:28 # Compute fields\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: nan_treatment='interpolate', however, NaN values detected post convolution. A contiguous region of NaN values, larger than the kernel size, are present in the input array. Increase the kernel size to avoid this. [astropy.convolution.convolve]\n",
      "WARNING: nan_treatment='interpolate', however, NaN values detected post convolution. A contiguous region of NaN values, larger than the kernel size, are present in the input array. Increase the kernel size to avoid this. [astropy.convolution.convolve]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-02-14 01:59:28 # DONE\n",
      "2020-02-14 02:59:12 # Start\n",
      "2020-02-14 02:59:12 # Compute dask\n",
      "2020-02-14 02:59:12 # Compute fields\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: nan_treatment='interpolate', however, NaN values detected post convolution. A contiguous region of NaN values, larger than the kernel size, are present in the input array. Increase the kernel size to avoid this. [astropy.convolution.convolve]\n",
      "WARNING: nan_treatment='interpolate', however, NaN values detected post convolution. A contiguous region of NaN values, larger than the kernel size, are present in the input array. Increase the kernel size to avoid this. [astropy.convolution.convolve]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-02-14 02:59:12 # DONE\n",
      "2020-02-14 03:58:56 # Start\n",
      "2020-02-14 03:58:56 # Compute dask\n",
      "2020-02-14 03:58:56 # Compute fields\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: nan_treatment='interpolate', however, NaN values detected post convolution. A contiguous region of NaN values, larger than the kernel size, are present in the input array. Increase the kernel size to avoid this. [astropy.convolution.convolve]\n",
      "WARNING: nan_treatment='interpolate', however, NaN values detected post convolution. A contiguous region of NaN values, larger than the kernel size, are present in the input array. Increase the kernel size to avoid this. [astropy.convolution.convolve]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-02-14 03:58:56 # DONE\n",
      "2020-02-14 05:00:48 # Start\n",
      "2020-02-14 05:00:48 # Compute dask\n",
      "2020-02-14 05:00:48 # Compute fields\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: nan_treatment='interpolate', however, NaN values detected post convolution. A contiguous region of NaN values, larger than the kernel size, are present in the input array. Increase the kernel size to avoid this. [astropy.convolution.convolve]\n",
      "WARNING: nan_treatment='interpolate', however, NaN values detected post convolution. A contiguous region of NaN values, larger than the kernel size, are present in the input array. Increase the kernel size to avoid this. [astropy.convolution.convolve]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Save the fields\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m friday \u001b[38;5;129;01min\u001b[39;00m fridays:\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     \u001b[43msave_friday_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfriday\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 559\u001b[39m, in \u001b[36msave_friday_data\u001b[39m\u001b[34m(ds, friday)\u001b[39m\n\u001b[32m    556\u001b[39m _, bhl_prime, _ = compute_SST_var_derivative_fields(sst_deriv,wind_deriv,submeso_pass_lower_sigma,llon,llat,fine_sst,fine_10u,fine_10v,fine_blh)\n\u001b[32m    558\u001b[39m smooth_tas = gm.nan_gaussian_filter(fine_tas,submeso_pass_lower_sigma)\n\u001b[32m--> \u001b[39m\u001b[32m559\u001b[39m smooth_sst = \u001b[43mgm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnan_gaussian_filter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfine_sst\u001b[49m\u001b[43m,\u001b[49m\u001b[43msubmeso_pass_lower_sigma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    561\u001b[39m output = {\n\u001b[32m    562\u001b[39m     sst_deriv : sst_prime.flatten(),\n\u001b[32m    563\u001b[39m     wind_deriv : wind_prime.flatten(),\n\u001b[32m   (...)\u001b[39m\u001b[32m    567\u001b[39m     \u001b[33m'\u001b[39m\u001b[33msmooth_sst\u001b[39m\u001b[33m'\u001b[39m : smooth_sst.flatten(),\n\u001b[32m    568\u001b[39m }\n\u001b[32m    570\u001b[39m output_pd = pd.DataFrame.from_dict(output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Scripts/lfdavoli/geometry.py:123\u001b[39m, in \u001b[36mnan_gaussian_filter\u001b[39m\u001b[34m(field, sigma)\u001b[39m\n\u001b[32m    120\u001b[39m kernel = Gaussian2DKernel(x_stddev=sigma)\n\u001b[32m    122\u001b[39m \u001b[38;5;66;03m# Apply convolution (Gaussian filter)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconvolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfield\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mboundary\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mfill\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnan\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/easy/lib/python3.12/site-packages/astropy/nddata/decorators.py:254\u001b[39m, in \u001b[36msupport_nddata.<locals>.support_nddata_decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    251\u001b[39m \u001b[38;5;66;03m# Need to apply defaults so that any positional only arguments are\u001b[39;00m\n\u001b[32m    252\u001b[39m \u001b[38;5;66;03m# passed as positional properly\u001b[39;00m\n\u001b[32m    253\u001b[39m bound_args.apply_defaults()\n\u001b[32m--> \u001b[39m\u001b[32m254\u001b[39m result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mbound_args\u001b[49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mbound_args\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m unpack \u001b[38;5;129;01mand\u001b[39;00m repack:\n\u001b[32m    257\u001b[39m     \u001b[38;5;66;03m# If there are multiple required returned arguments make sure\u001b[39;00m\n\u001b[32m    258\u001b[39m     \u001b[38;5;66;03m# the result is a tuple (because we don't want to unpack\u001b[39;00m\n\u001b[32m    259\u001b[39m     \u001b[38;5;66;03m# numpy arrays or compare their length, never!) and has the\u001b[39;00m\n\u001b[32m    260\u001b[39m     \u001b[38;5;66;03m# same length.\u001b[39;00m\n\u001b[32m    261\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(returns) > \u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/easy/lib/python3.12/site-packages/astropy/convolution/convolve.py:407\u001b[39m, in \u001b[36mconvolve\u001b[39m\u001b[34m(array, kernel, boundary, fill_value, nan_treatment, normalize_kernel, mask, preserve_nan, normalization_zero_tol)\u001b[39m\n\u001b[32m    401\u001b[39m             np_pad_width = ((pad_width[\u001b[32m0\u001b[39m],), (pad_width[\u001b[32m1\u001b[39m],), (pad_width[\u001b[32m2\u001b[39m],))\n\u001b[32m    403\u001b[39m         array_to_convolve = np.pad(\n\u001b[32m    404\u001b[39m             array_internal, pad_width=np_pad_width, mode=np_pad_mode\n\u001b[32m    405\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m407\u001b[39m \u001b[43m_convolveNd_c\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    408\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    409\u001b[39m \u001b[43m    \u001b[49m\u001b[43marray_to_convolve\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkernel_internal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnan_interpolate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    412\u001b[39m \u001b[43m    \u001b[49m\u001b[43membed_result_within_padded_region\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    413\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    414\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[38;5;66;03m# So far, normalization has only occurred for nan_treatment == 'interpolate'\u001b[39;00m\n\u001b[32m    417\u001b[39m \u001b[38;5;66;03m# because this had to happen within the C extension so as to ignore\u001b[39;00m\n\u001b[32m    418\u001b[39m \u001b[38;5;66;03m# any NaNs\u001b[39;00m\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m normalize_kernel:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "with cat[model](zoom=fine_zoom,time=time_res).to_dask() as ds:\n",
    "    # Choose only the best day for each week (ref. Giaccio et. al, PREPRINT)\n",
    "    times = pd.to_datetime(ds.time.data)\n",
    "    fridays = times[(times.weekday == 4) & (times>pd.to_datetime('2020-02-07T13:00:00')) & (times<pd.to_datetime('2020-03-01'))] \n",
    "    # Save the fields\n",
    "    for friday in fridays:\n",
    "        save_friday_data(ds,friday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6939cfb6-708e-452b-8706-340759e70e9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "global-hackathon-easy",
   "language": "python",
   "name": "global-hackathon-easy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
